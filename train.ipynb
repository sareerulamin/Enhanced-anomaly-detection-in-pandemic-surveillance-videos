{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "915924e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import import_ipynb\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "#from models import  resnet_v1, resnet_v2, mobilenets, inception_v3, inception_resnet_v2, densenet\n",
    "import efficientnet.tfkeras as efn\n",
    "#from utils import lr_schedule\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "674621db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "122f7cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "num_classes = 8\n",
    "num_of_test_samples =4260 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f701d415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class SpatialAttentionModule(tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel_size=3):\n",
    "       \n",
    "        super(SpatialAttentionModule, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, kernel_size=kernel_size, \n",
    "                                            use_bias=False, \n",
    "                                            kernel_initializer='he_normal',\n",
    "                                            strides=1, padding='same', \n",
    "                                            activation=tf.nn.relu6)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(32, kernel_size=kernel_size, \n",
    "                                            use_bias=False, \n",
    "                                            kernel_initializer='he_normal',\n",
    "                                            strides=1, padding='same', \n",
    "                                            activation=tf.nn.relu6)\n",
    "        self.conv3 = tf.keras.layers.Conv2D(16, kernel_size=kernel_size, \n",
    "                                            use_bias=False, \n",
    "                                            kernel_initializer='he_normal',\n",
    "                                            strides=1, padding='same', \n",
    "                                            activation=tf.nn.relu6)\n",
    "        self.conv4 = tf.keras.layers.Conv2D(1, kernel_size=kernel_size,  \n",
    "                                            use_bias=False,\n",
    "                                            kernel_initializer='he_normal',\n",
    "                                            strides=1, padding='same', \n",
    "                                            activation=tf.math.sigmoid)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        avg_out = tf.reduce_mean(inputs, axis=3)\n",
    "        max_out = tf.reduce_max(inputs,  axis=3)\n",
    "        x = tf.stack([avg_out, max_out], axis=3) \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return self.conv4(x)\n",
    "    \n",
    "# A custom layer\n",
    "class ChannelAttentionModule(tf.keras.layers.Layer):\n",
    "    def __init__(self, ratio=8):\n",
    "       \n",
    "        super(ChannelAttentionModule, self).__init__()\n",
    "        self.ratio = ratio\n",
    "        self.gapavg = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.gmpmax = tf.keras.layers.GlobalMaxPooling2D()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.conv1 = tf.keras.layers.Conv2D(input_shape[-1]//self.ratio, \n",
    "                                            kernel_size=1, \n",
    "                                            strides=1, padding='same',\n",
    "                                            use_bias=True, activation=tf.nn.relu)\n",
    "    \n",
    "        self.conv2 = tf.keras.layers.Conv2D(input_shape[-1], \n",
    "                                            kernel_size=1, \n",
    "                                            strides=1, padding='same',\n",
    "                                            use_bias=True, activation=tf.nn.relu)\n",
    "        super(ChannelAttentionModule, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # compute gap and gmp pooling \n",
    "        gapavg = self.gapavg(inputs)\n",
    "        gmpmax = self.gmpmax(inputs)\n",
    "        gapavg = tf.keras.layers.Reshape((1, 1, gapavg.shape[1]))(gapavg)   \n",
    "        gmpmax = tf.keras.layers.Reshape((1, 1, gmpmax.shape[1]))(gmpmax)   \n",
    "        # forward passing to the respected layers\n",
    "        gapavg_out = self.conv2(self.conv1(gapavg))\n",
    "        gmpmax_out = self.conv2(self.conv1(gmpmax))\n",
    "        return tf.math.sigmoid(gapavg_out + gmpmax_out)\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[3]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "class AttentionWeightedAverage2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = tf.keras.initializers.get('uniform')\n",
    "        super(AttentionWeightedAverage2D, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [tf.keras.layers.InputSpec(ndim=4)]\n",
    "        assert len(input_shape) == 4\n",
    "        self.W = self.add_weight(shape=(input_shape[3], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self._trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage2D, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 2-dimensional weights\n",
    "        logits  =tf.linalg.matmul(x, self.W)\n",
    "        x_shape = tf.shape(x)\n",
    "        logits  = tf.reshape(logits, (x_shape[0], x_shape[1], x_shape[2]))\n",
    "        ai      = tf.exp(logits - tf.reduce_max(logits, axis=[1,2], keepdims=True))\n",
    "        \n",
    "        att_weights    = ai / (tf.reduce_sum(ai, axis=[1,2], keepdims=True) + tf.keras.backend.epsilon())\n",
    "        weighted_input = x * tf.expand_dims(att_weights, axis=-1)\n",
    "        result         = tf.reduce_sum(weighted_input, axis=[1,2])\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[3]\n",
    "        return (input_shape[0], output_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f2ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB1\n",
    "\n",
    "\n",
    "#build the model\n",
    "effnet = EfficientNetB1(weights='imagenet', \n",
    "                        include_top=False, \n",
    "                        input_shape=(224, 224, 3))\n",
    "\n",
    "x = effnet.output\n",
    "CAN  = ChannelAttentionModule()\n",
    "SPN = SpatialAttentionModule()\n",
    "AWG  = AttentionWeightedAverage2D()\n",
    "canx   = CAN(x)*x\n",
    "spnx   = SPN(canx)*canx\n",
    "spny   = SPN(canx)\n",
    "gapx = tf.keras.layers.GlobalAveragePooling2D()(spnx)\n",
    "gapy = tf.keras.layers.GlobalAveragePooling2D()(spnx)\n",
    "\n",
    "\n",
    "avg = tf.keras.layers.Average()([gapx, gapy])\n",
    "\n",
    "awg = AWG(x)\n",
    "x = tf.keras.layers.Add()([avg, awg])\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "\n",
    "x = tf.keras.layers.Dense(512, activation = 'relu')(x)\n",
    "x = tf.keras.layers.Dense(num_classes,activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=effnet.input,outputs=x )\n",
    "\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d0236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize the model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b5129b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91c8bc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b57ef046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "#opt = SGD(lr=0.00001, momentum=0.9)\n",
    "#model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9edb605",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(validation_split=0.3)\n",
    "\n",
    "train_generator = datagen.flow_from_directory('dataset', batch_size=batch_size, target_size=(224, 224), subset='training')\n",
    "validation_generator = datagen.flow_from_directory('dataset', batch_size=batch_size, target_size=(224, 224), shuffle=False, subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bf4a63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "start_time = time.perf_counter()\n",
    "#steps_per_epoch = 66\n",
    "#validation_steps = 66\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                              \n",
    "                              epochs=epochs,\n",
    "                            \n",
    "                              validation_data=validation_generator,\n",
    "                              #callbacks=[tensorboard_callback],\n",
    "                              verbose=1\n",
    "                             )\n",
    "\n",
    "# serialize weights to HDF5\n",
    "#model.save(\"saved_cbam_eff0_model\")\n",
    "#print(\"Saved model to disk\")\n",
    "print(time.perf_counter()-start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2a29a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define labels for testing\n",
    "y_test = validation_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make prediction\n",
    "yhat_test = np.argmax(model.predict(validation_generator), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c088ab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2954e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9f5ed4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import random\n",
    "from IPython.display import Image\n",
    "import imutils   \n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix, classification_report\n",
    "\n",
    "import keras\n",
    "import tensorflow.keras as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator, array_to_img, img_to_array\n",
    "from tensorflow.keras.applications import EfficientNetB1\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv2D, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import imutils    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34abc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "last_conv_layer = next(x for x in model.layers[::-1] if isinstance(x, K.layers.Conv2D))\n",
    "last_conv_layer.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e66e39a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/gkeechin/vizgradcam/blob/main/gradcam.py\n",
    "\n",
    "def VizGradCAM(model, image, interpolant=0.5, plot_results=True):\n",
    "\n",
    "    #sanity check\n",
    "    assert (interpolant > 0 and interpolant < 1), \"Heatmap Interpolation Must Be Between 0 - 1\"\n",
    "\n",
    "    #STEP 1: Preprocesss image and make prediction using our model\n",
    "    #input image\n",
    "    original_img = np.asarray(image, dtype = np.float32)\n",
    "    #expamd dimension and get batch size\n",
    "    img = np.expand_dims(original_img, axis=0)\n",
    "    #predict\n",
    "    prediction = model.predict(img)\n",
    "    #prediction index\n",
    "    prediction_idx = np.argmax(prediction)\n",
    "\n",
    "    #STEP 2: Create new model\n",
    "    #specify last convolutional layer\n",
    "    last_conv_layer = next(x for x in model.layers[::-1] if isinstance(x, K.layers.Conv2D))\n",
    "    target_layer = model.get_layer(last_conv_layer.name)\n",
    "\n",
    "    #compute gradient of top predicted class\n",
    "    with tf.GradientTape() as tape:\n",
    "        #create a model with original model inputs and the last conv_layer as the output\n",
    "        gradient_model = Model([model.inputs], [target_layer.output, model.output])\n",
    "        #pass the image through the base model and get the feature map  \n",
    "        conv2d_out, prediction = gradient_model(img)\n",
    "        #prediction loss\n",
    "        loss = prediction[:, prediction_idx]\n",
    "\n",
    "    #gradient() computes the gradient using operations recorded in context of this tape\n",
    "    gradients = tape.gradient(loss, conv2d_out)\n",
    "\n",
    "    #obtain the output from shape [1 x H x W x CHANNEL] -> [H x W x CHANNEL]\n",
    "    output = conv2d_out[0]\n",
    "\n",
    "    #obtain depthwise mean\n",
    "    weights = tf.reduce_mean(gradients[0], axis=(0, 1))\n",
    "\n",
    "\n",
    "    #create a 7x7 map for aggregation\n",
    "    activation_map = np.zeros(output.shape[0:2], dtype=np.float32)\n",
    "    #multiply weight for every layer\n",
    "    for idx, weight in enumerate(weights):\n",
    "        activation_map += weight * output[:, :, idx]\n",
    "    #resize to image size\n",
    "    activation_map = cv2.resize(activation_map.numpy(), \n",
    "                                (original_img.shape[1], \n",
    "                                 original_img.shape[0]))\n",
    "    #ensure no negative number\n",
    "    activation_map = np.maximum(activation_map, 0)\n",
    "    #convert class activation map to 0 - 255\n",
    "    activation_map = (activation_map - activation_map.min()) / (activation_map.max() - activation_map.min())\n",
    "    #rescale and convert the type to int\n",
    "    activation_map = np.uint8(255 * activation_map)\n",
    "\n",
    "\n",
    "    #convert to heatmap\n",
    "    heatmap = cv2.applyColorMap(activation_map, cv2.COLORMAP_JET)\n",
    "\n",
    "    #superimpose heatmap onto image\n",
    "    original_img = np.uint8((original_img - original_img.min()) / (original_img.max() - original_img.min()) * 255)\n",
    "    cvt_heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    cvt_heatmap = img_to_array(cvt_heatmap)\n",
    "\n",
    "    #enlarge plot\n",
    "    plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "    if plot_results == True:\n",
    "        plt.imshow(np.uint8(original_img * interpolant + cvt_heatmap * (1 - interpolant)))\n",
    "    else:\n",
    "        return cvt_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b65d829",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read an image\n",
    "img = cv2.imread('test_gradcam/Picture8.png')\n",
    "width = 224\n",
    "height = 224\n",
    "dim = (width, height)\n",
    "\n",
    "\n",
    "resized_img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a79697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply function\n",
    "VizGradCAM(model, img_to_array(resized_img), plot_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4525b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.plot([None] + history.history['accuracy'], 'o-')\n",
    "ax.plot([None] + history.history['val_accuracy'], 'x-')\n",
    "# Plot legend and use the best location automatically: loc = 0.\n",
    "ax.legend(['Train acc', 'Validation acc'], loc = 0)\n",
    "ax.set_title('Training/Validation acc per Epoch')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.plot([None] + history.history['loss'], 'o-')\n",
    "ax.plot([None] + history.history['val_loss'], 'x-')\n",
    "\n",
    "# Plot legend and use the best location automatically: loc = 0.\n",
    "ax.legend(['Train loss', \"Val loss\"], loc = 1)\n",
    "ax.set_title('Training/Validation Loss per Epoch')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3066ab62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size=batch_size\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "num_of_test_samples = 4260 \n",
    "target_names = [\"Coughing\",\"Face_Mask\",\"No_mask\",\"Nose_picking\",\"sneezing\",\"spitting\",\"wrong_mask\",\"yawn\"] \n",
    "#Confution Matrix and Classification Report\n",
    "Y_pred = model.predict_generator(validation_generator, num_of_test_samples // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "#print('Confusion Matrix')\n",
    "cm = confusion_matrix(validation_generator.classes, y_pred)\n",
    "#print(cm)\n",
    "print('Classification Report')\n",
    "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n",
    "# Normalise\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)\n",
    "fig, ax = plt.subplots(figsize=(20,7))\n",
    "\n",
    "sns.heatmap(cmn, center=0, annot=True, fmt='.2f', linewidths=1,  xticklabels=target_names, yticklabels=target_names)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show(block=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1ffae8",
   "metadata": {},
   "source": [
    "# INFERENCING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a50c9af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model = keras.models.load_model(\"saved_cbam_eff0_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a908c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcedde94",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "reconstructed_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3fb687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make prediction\n",
    "yhat_test = np.argmax(reconstructed_model.predict(validation_generator), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0818f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3c9883",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb20c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size=batch_size\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "num_of_test_samples = 4260 \n",
    "target_names = [\"Coughing\",\"Face_Mask\",\"No_mask\",\"Nose_picking\",\"sneezing\",\"spitting\",\"wrong_mask\",\"yawn\"] \n",
    "#Confution Matrix and Classification Report\n",
    "Y_pred = reconstructed_model.predict_generator(validation_generator, num_of_test_samples // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "#print('Confusion Matrix')\n",
    "cm = confusion_matrix(validation_generator.classes, y_pred)\n",
    "#print(cm)\n",
    "print('Classification Report')\n",
    "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n",
    "# Normalise\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)\n",
    "fig, ax = plt.subplots(figsize=(20,7))\n",
    "\n",
    "sns.heatmap(cmn, center=0, annot=True, fmt='.2f', linewidths=1,  xticklabels=target_names, yticklabels=target_names)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show(block=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0facb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_conv_layer = next(x for x in reconstructed_model.layers[::-1] if isinstance(x, K.layers.Conv2D))\n",
    "last_conv_layer.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d624cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/gkeechin/vizgradcam/blob/main/gradcam.py\n",
    "\n",
    "def VizGradCAM(model, image, interpolant=0.5, plot_results=True):\n",
    "\n",
    "    \"\"\"VizGradCAM - Displays GradCAM based on Keras / TensorFlow models\n",
    "    using the gradients from the last convolutional layer. This function\n",
    "    should work with all Keras Application listed here:\n",
    "    https://keras.io/api/applications/\n",
    "    Parameters:\n",
    "    model (keras.model): Compiled Model with Weights Loaded\n",
    "    image: Image to Perform Inference On\n",
    "    plot_results (boolean): True - Function Plots using PLT\n",
    "                            False - Returns Heatmap Array\n",
    "    Returns:\n",
    "    Heatmap Array?\n",
    "    \"\"\"\n",
    "    #sanity check\n",
    "    assert (interpolant > 0 and interpolant < 1), \"Heatmap Interpolation Must Be Between 0 - 1\"\n",
    "\n",
    "    #STEP 1: Preprocesss image and make prediction using our model\n",
    "    #input image\n",
    "    original_img = np.asarray(image, dtype = np.float32)\n",
    "    #expamd dimension and get batch size\n",
    "    img = np.expand_dims(original_img, axis=0)\n",
    "    #predict\n",
    "    prediction = model.predict(img)\n",
    "    #prediction index\n",
    "    prediction_idx = np.argmax(prediction)\n",
    "\n",
    "    #STEP 2: Create new model\n",
    "    #specify last convolutional layer\n",
    "    last_conv_layer = next(x for x in model.layers[::-1] if isinstance(x, K.layers.Conv2D))\n",
    "    target_layer = model.get_layer(last_conv_layer.name)\n",
    "\n",
    "    #compute gradient of top predicted class\n",
    "    with tf.GradientTape() as tape:\n",
    "        #create a model with original model inputs and the last conv_layer as the output\n",
    "        gradient_model = Model([model.inputs], [target_layer.output, model.output])\n",
    "        #pass the image through the base model and get the feature map  \n",
    "        conv2d_out, prediction = gradient_model(img)\n",
    "        #prediction loss\n",
    "        loss = prediction[:, prediction_idx]\n",
    "\n",
    "    #gradient() computes the gradient using operations recorded in context of this tape\n",
    "    gradients = tape.gradient(loss, conv2d_out)\n",
    "\n",
    "    #obtain the output from shape [1 x H x W x CHANNEL] -> [H x W x CHANNEL]\n",
    "    output = conv2d_out[0]\n",
    "\n",
    "    #obtain depthwise mean\n",
    "    weights = tf.reduce_mean(gradients[0], axis=(0, 1))\n",
    "\n",
    "\n",
    "    #create a 7x7 map for aggregation\n",
    "    activation_map = np.zeros(output.shape[0:2], dtype=np.float32)\n",
    "    #multiply weight for every layer\n",
    "    for idx, weight in enumerate(weights):\n",
    "        activation_map += weight * output[:, :, idx]\n",
    "    #resize to image size\n",
    "    activation_map = cv2.resize(activation_map.numpy(), \n",
    "                                (original_img.shape[1], \n",
    "                                 original_img.shape[0]))\n",
    "    #ensure no negative number\n",
    "    activation_map = np.maximum(activation_map, 0)\n",
    "    #convert class activation map to 0 - 255\n",
    "    activation_map = (activation_map - activation_map.min()) / (activation_map.max() - activation_map.min())\n",
    "    #rescale and convert the type to int\n",
    "    activation_map = np.uint8(255 * activation_map)\n",
    "\n",
    "\n",
    "    #convert to heatmap\n",
    "    heatmap = cv2.applyColorMap(activation_map, cv2.COLORMAP_JET)\n",
    "\n",
    "    #superimpose heatmap onto image\n",
    "    original_img = np.uint8((original_img - original_img.min()) / (original_img.max() - original_img.min()) * 255)\n",
    "    cvt_heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    cvt_heatmap = img_to_array(cvt_heatmap)\n",
    "\n",
    "    #enlarge plot\n",
    "    plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "    if plot_results == True:\n",
    "        plt.imshow(np.uint8(original_img * interpolant + cvt_heatmap * (1 - interpolant)))\n",
    "    else:\n",
    "        return cvt_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c821d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read an image\n",
    "img = cv2.imread('dataset/Coughing/3.jpg')\n",
    "width = 224\n",
    "height = 224\n",
    "dim = (width, height)\n",
    "\n",
    "\n",
    "resized_img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cae508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply function\n",
    "VizGradCAM(reconstructed_model, img_to_array(resized_img), plot_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0b58c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
